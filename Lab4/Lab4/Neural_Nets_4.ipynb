{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural_Nets_4.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "e8ZclteNv4_s",
        "AJ9dEh-AbQV3",
        "RtLVjHRCbQWI",
        "kRnPB84SEDYT",
        "aX8jmI3KbQWJ",
        "R-rfPmhFbQWK",
        "0g7wigiBbQWL",
        "8iYmPtstbQWN",
        "bPf-jF2NnAmZ",
        "uK1fdCEvnuKE",
        "2AaLWpGWpR_A",
        "8btcrXLRqUwI",
        "2wZKx1ZIq2mQ",
        "WVr7MDa-gco5",
        "AEtQL2OHC05p",
        "tWG5Hv4bvkfO",
        "2s33hmfYxUJS",
        "wkIiiuSPz7_f",
        "D6NyKskLghP6"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 1266.29551,
      "end_time": "2021-02-15T20:14:51.081780",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-02-15T19:53:44.786270",
      "version": "2.2.2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_wQhsTNAuVN"
      },
      "source": [
        "# Στοιχεία Ομάδας"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzGLgcQ4DMeJ"
      },
      "source": [
        "**Αριθμός Ομάδας:** 66\n",
        "\n",
        "**Ονοματεπώνυμα και ΑΜ:**\n",
        "\n",
        "Τσιλιβής Θεόδωρος 03116032\n",
        "\n",
        "Στόικου Θεοδότη 03117085\n",
        "\n",
        "Ποταμίτου Νεφέλη 03117709"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8ZclteNv4_s"
      },
      "source": [
        "# Imports and stuff\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0-3rar1v4Ey"
      },
      "source": [
        "LOG_DIR = 'tb_log/'\r\n",
        "!mkdir -p LOG_DIR\r\n",
        "%load_ext tensorboard\r\n",
        "# %tensorboard --logdir {LOG_DIR}\r\n",
        "!pip install --upgrade pip\r\n",
        "!pip install --upgrade stable_baselines3[extra]\r\n",
        "# we need a specific version of gym because of this issue: https://github.com/DLR-RM/stable-baselines3/issues/294\r\n",
        "!pip install gym==0.17.3\r\n",
        "atari_env_name='Berzerk-v0'\r\n",
        "from stable_baselines3.common.env_util import make_atari_env\r\n",
        "from stable_baselines3.common.vec_env import VecFrameStack\r\n",
        "import datetime # For filenames while logging\r\n",
        "from stable_baselines3 import DQN\r\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\r\n",
        "from stable_baselines3 import A2C\r\n",
        "from stable_baselines3 import PPO\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7XkjmzZwblk"
      },
      "source": [
        "!apt-get install ffmpeg freeglut3-dev xvfb  -y # For visualization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJxKCLOTwbll"
      },
      "source": [
        "# Set up fake display; otherwise rendering will fail\n",
        "import os\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67D2QRdUwblm"
      },
      "source": [
        "video_folder = '/videos'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjCiofJIwblm"
      },
      "source": [
        "import base64\n",
        "from pathlib import Path\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "def show_videos(video_path='', prefix=''):\n",
        "  \"\"\"\n",
        "  Taken from https://github.com/eleurent/highway-env\n",
        "\n",
        "  :param video_path: (str) Path to the folder containing videos\n",
        "  :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
        "  \"\"\"\n",
        "  html = []\n",
        "  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "      video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "      html.append('''<video alt=\"{}\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_kZnSvdwblm"
      },
      "source": [
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "\n",
        "def record_video(eval_env, model, video_length=500, prefix='', video_folder=video_folder):\n",
        "  \"\"\"\n",
        "  :param env_id: (str)\n",
        "  :param model: (RL model)\n",
        "  :param video_length: (int)\n",
        "  :param prefix: (str)\n",
        "  :param video_folder: (str)\n",
        "  \"\"\"\n",
        "  # Start the video at step=0 and record 500 steps\n",
        "  eval_env = VecVideoRecorder(eval_env, video_folder=video_folder,\n",
        "                              record_video_trigger=lambda step: step == 0, video_length=video_length,\n",
        "                              name_prefix=prefix)\n",
        "\n",
        "  obs = eval_env.reset()\n",
        "  for _ in range(video_length):\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    obs, _, _, _ = eval_env.step(action)\n",
        "\n",
        "  # Close the video recorder\n",
        "  eval_env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ9dEh-AbQV3"
      },
      "source": [
        "# Παίζοντας Atari με βαθιά ενισχυτική μάθηση\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtLVjHRCbQWI"
      },
      "source": [
        "## Tensorboard\n",
        "Ορίζουμε το directory με τα log files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wr97sGbsEEch"
      },
      "source": [
        "LOG_DIR = 'tb_log/'\n",
        "!mkdir -p LOG_DIR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRnPB84SEDYT"
      },
      "source": [
        "\n",
        "\n",
        "### Colab magic\n",
        "στο Colab δεν χρειάζεται το ngrok γιατί υπάρχει ειδικό magic για το TensorBoard.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cra2Y18SzQ1b"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {LOG_DIR}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPPRQdfzDpmb"
      },
      "source": [
        "```\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {LOG_DIR}\n",
        "```\n",
        "Σε εμάς δεν λειτούργησε, ωστόσο μας αναφέρθηκε ότι δουλεύει αν τρέξετε την πρώτη γραμμή πριν την εκπαίδευση και την δεύτερη μετά την εκπαίδευση (όταν έχει περιεχόμενο ο φάκελος LOG_DIR).\n",
        "\n",
        "Παρόλαυτά εμείς θέλουμε να βλέπουμε ζωντανά το TensorBoard κατά την εκπαίδευση και όχι μετά ώστε πχ να μπορούμε να διακόψουμε μοντέλα που δεν έχουν ικανοποιητικές γραφικές. Αυτό θέλουμε να το πετύχουμε χωρίς callbacks\n",
        ".\n",
        "\n",
        "Και πάλι προτείνουμε το Kaggle version.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX8jmI3KbQWJ"
      },
      "source": [
        "## Εγκατάσταση βιβλιοθήκης και gym"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AKNWi4ibQWJ"
      },
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install --upgrade stable_baselines3[extra]\n",
        "# we need a specific version of gym because of this issue: https://github.com/DLR-RM/stable-baselines3/issues/294\n",
        "!pip install gym==0.17.3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-rfPmhFbQWK"
      },
      "source": [
        "## Ορισμός παιχνιδιού\n",
        "\n",
        "Βάλτε εδώ το string που αντιστοιχεί στο παιχνίδι σας. Για την ονοματολογία των περιβαλλόντων:\n",
        "- v0 vs v4: v0 has repeat_action_probability of 0.25 (meaning 25% of the time the previous action will be used instead of the new action), while v4 has 0 (always follow your issued action)\n",
        "- Deterministic: a fixed frameskip of 4, while for the env without Deterministic, frameskip is sampled from [2,4]\n",
        "- There is also NoFrameskip-v4 with no frame skip and no action repeat stochasticity.\n",
        "\n",
        "Αναλυτικότερα στην εκφώνηση της άσκησης."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uby_Zr6rbQWK",
        "scrolled": false
      },
      "source": [
        "atari_env_name='Berzerk-v4'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0g7wigiBbQWL"
      },
      "source": [
        "## Δημιουργία περιβάλλοντος"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PV8ecQy2bQWL"
      },
      "source": [
        "from stable_baselines3.common.env_util import make_atari_env\n",
        "from stable_baselines3.common.vec_env import VecFrameStack"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9_wM8o5bQWL"
      },
      "source": [
        "# Με τις συναρτήσεις που ακολουθούν κάνουμε την ίδια προεπεξεργασία με την DeepMind\n",
        "\n",
        "# Here we are also multi-worker training (n_envs=4 => 4 environments), The model must support Multi Processing\n",
        "env = make_atari_env(atari_env_name, n_envs=1, seed=0)\n",
        "# Frame-stacking with 4 frames. Με 1 frame ο αλγόριθμος ξέρει τη θέση των πραγμάτων, με 2 frames την ταχύτητα, με 3 την επιτάχυνση και με 4 το jerk\n",
        "env = VecFrameStack(env, n_stack=4)\n",
        "# Test environment must be unique\n",
        "test_env = make_atari_env(atari_env_name, n_envs=1, seed=0)\n",
        "# Frame-stacking with 4 frames\n",
        "test_env = VecFrameStack(test_env, n_stack=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qP9rchebQWL"
      },
      "source": [
        "## Εκπαίδευση\n",
        "\n",
        "Θα εκπαιδεύσουμε ένα δίκτυο deep q-learning (DQN) όπως αυτό της Deepmind. Σημειώστε ότι τα timesteps είναι πολύ λίγα και ότι δεν κάνουμε διερεύνηση στις παραμέτρους του μοντέλου που μπορείτε να βρείτε [εδώ](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html#parameters)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fxo3jZ_ObQWM",
        "scrolled": false
      },
      "source": [
        "import datetime # For filenames while logging\r\n",
        "from stable_baselines3 import DQN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLtp13dJbQWM",
        "outputId": "c67e10b2-88df-4095-f49b-1bc9aa1628bf"
      },
      "source": [
        "\n",
        "\n",
        "model_name='dqn-MlpPolicy'\n",
        "time_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\n",
        "model_log= LOG_DIR + model_name + time_stamp\n",
        "\n",
        "dqn_model = DQN('MlpPolicy', env, verbose=1, tensorboard_log=model_log, buffer_size=100000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda device\n",
            "Wrapping the env in a VecTransposeImage.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXQTD_t1bQWM"
      },
      "source": [
        "max_steps=100\n",
        "dqn_model.learn(total_timesteps=max_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmA5j-9OzZy1"
      },
      "source": [
        "%tensorboard --logdir {LOG_DIR}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QoCc0wfbQWN"
      },
      "source": [
        "## Εκτίμηση απόδοσης"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3f-qoOKbQWN"
      },
      "source": [
        "from stable_baselines3.common.evaluation import evaluate_policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLU_RvfqbQWN"
      },
      "source": [
        "mean_reward, std_reward = evaluate_policy(dqn_model, test_env, n_eval_episodes=10)\n",
        "print(f\"Eval reward: {mean_reward} (+/-{std_reward})\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iYmPtstbQWN"
      },
      "source": [
        "## Σώσιμο εκπαιδευμένου μοντέλου"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSd9QsnQbQWO"
      },
      "source": [
        "dqn_model.save(\"dqn_pong\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiKP0AAObQWO"
      },
      "source": [
        "Το μοντέλο θα αποθηκευθεί ως zip και μπορείτε να το κατεβάσετε τοπικά από το αριστερό sidebar του Colab στο \"Files\" και μετά στο ellipsis menu πάνω στο filename."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O4tFU0CbQWO"
      },
      "source": [
        "## Φόρτωση εκπαιδευμένου μοντέλου\n",
        "\n",
        "Από το αριστερό sidebar του Colab και το \"Files\" ανεβάστε το αρχείο zip του εκπαιδευμένου μοντέλου.\n",
        "\n",
        "Εδώ θα ανεβάσουμε ένα μοντέλο Α2C που έχουμε εκπαιδεύσει νωρίτερα. Μπορείτε να το κατεβάσετε από [εδώ](https://drive.google.com/uc?export=download&id=1COsaNOH8SjbpxxIYc5lOF-QUiUJGU5ZB). \n",
        "\n",
        "Αν χρειαστεί μετονομάστε το αρχείο σε a2c_pong.zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dq0gCqbibQWP"
      },
      "source": [
        "from stable_baselines3 import A2C\n",
        "# !wget --no-check-certificate https://www.dropbox.com/s/zm02848gzbx3jsl/a2c_pong.zip?dl=1 -O a2c_berzerk.zip\n",
        "# a2c_model = A2C.load(\"a2c_berzerk.zip\", verbose=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zz9Ew8GWOtg0",
        "outputId": "b54c1376-9e71-4dca-fd45-0d0f6ca38aa8"
      },
      "source": [
        "model_name='a2c-MlpPolicy'\r\n",
        "time_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\r\n",
        "model_log= LOG_DIR + model_name + time_stamp\r\n",
        "\r\n",
        "a2c_model = A2C('MlpPolicy', env, verbose=1, tensorboard_log=model_log)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda device\n",
            "Wrapping the env in a VecTransposeImage.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5IQo_cvPXoy"
      },
      "source": [
        "max_steps=10000\r\n",
        "a2c_model.learn(total_timesteps=max_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hAVkj_2Pn_2"
      },
      "source": [
        "%tensorboard --logdir {LOG_DIR}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJwSrikKbQWP"
      },
      "source": [
        "## Συνέχιση της εκπαίδευσης"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOL8CLwmbQWP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3b623b4-2230-4870-fe3d-97a316807a6e"
      },
      "source": [
        "# Προσοχή, το n_envs πρέπει να είναι πάντα το ίδιο (αυτό με το οποίο έγινε η αρχική εκπαίδευση του μοντέλου)\n",
        "new_env = make_atari_env(atari_env_name, n_envs=4, seed=0)\n",
        "# Frame-stacking with 4 frames\n",
        "new_env = VecFrameStack(new_env, n_stack=4)\n",
        "# Το μοντέλο δεν έχει μαζί του το περιβάλλον, πρέπει να του το αναθέσουμε ξανά.\n",
        "a2c_model.set_env(new_env)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wrapping the env in a VecTransposeImage.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAG1UOMmbQWP"
      },
      "source": [
        "#max_steps=10000\n",
        "#a2c_model.learn(total_timesteps=max_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ai-pQBRqbQWQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85439d23-bb04-4944-9554-70ea0959d787"
      },
      "source": [
        "mean_reward, std_reward = evaluate_policy(a2c_model, test_env, n_eval_episodes=10)\n",
        "print(f\"Eval reward: {mean_reward} (+/-{std_reward})\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Eval reward: 365.0 (+/-122.57650672131263)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4Vf0lN7bQWQ"
      },
      "source": [
        "a2c_model.save(\"a2c_pong\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEPM6rytbQWQ"
      },
      "source": [
        "## Καταγραφή video του actual gameplay του πράκτορα\n",
        "\n",
        "Ωραία τα διαγράμματα, ωραίο το TensorBoard, αλλά θέλουμε οπωσδήποτε να δούμε τον πράκτορα να παίζει το παιχνίδι!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7lbtGM7bQWQ"
      },
      "source": [
        "!apt-get install ffmpeg freeglut3-dev xvfb  -y # For visualization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4Iwzn5YbQWR"
      },
      "source": [
        "# Set up fake display; otherwise rendering will fail\n",
        "import os\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tkys6SPFbQWR"
      },
      "source": [
        "video_folder = '/videos'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaIM29K8bQWR"
      },
      "source": [
        "import base64\n",
        "from pathlib import Path\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "def show_videos(video_path='', prefix=''):\n",
        "  \"\"\"\n",
        "  Taken from https://github.com/eleurent/highway-env\n",
        "\n",
        "  :param video_path: (str) Path to the folder containing videos\n",
        "  :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
        "  \"\"\"\n",
        "  html = []\n",
        "  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "      video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "      html.append('''<video alt=\"{}\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EYx2HFmbQWR"
      },
      "source": [
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "\n",
        "def record_video(eval_env, model, video_length=500, prefix='', video_folder=video_folder):\n",
        "  \"\"\"\n",
        "  :param env_id: (str)\n",
        "  :param model: (RL model)\n",
        "  :param video_length: (int)\n",
        "  :param prefix: (str)\n",
        "  :param video_folder: (str)\n",
        "  \"\"\"\n",
        "  # Start the video at step=0 and record 500 steps\n",
        "  eval_env = VecVideoRecorder(eval_env, video_folder=video_folder,\n",
        "                              record_video_trigger=lambda step: step == 0, video_length=video_length,\n",
        "                              name_prefix=prefix)\n",
        "\n",
        "  obs = eval_env.reset()\n",
        "  for _ in range(video_length):\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    obs, _, _, _ = eval_env.step(action)\n",
        "\n",
        "  # Close the video recorder\n",
        "  eval_env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ou9ywjHoFoVW"
      },
      "source": [
        "# record_video(test_env, dqn_model, video_length=5000, prefix='dqn_pong')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFP7n5eabQWS"
      },
      "source": [
        "record_video(test_env, a2c_model, video_length=5000, prefix='a2c_pong')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuIOgfWcFsiU"
      },
      "source": [
        "# show_videos(video_path = video_folder, prefix='dqn')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8ad64b-bQWT"
      },
      "source": [
        "show_videos(video_path = video_folder, prefix='a2c')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VSs5uyWSEyw"
      },
      "source": [
        "Μπορείτε να κατεβάσετε το video με hover πάνω του, κάτω δεξιά στο ellipsis menu και \"Download\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFlVwmXxn3vv"
      },
      "source": [
        "# Structured approach\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eHXgtCD2boL"
      },
      "source": [
        "## DQN Deterministic-v4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c52bUnI9n6Pd"
      },
      "source": [
        "# -v4, Deterministic\r\n",
        "atari_env_name='BerzerkDeterministic-v4'\r\n",
        "\r\n",
        "# Here we are also multi-worker training (n_envs=4 => 4 environments), The model must support Multi Processing\r\n",
        "env = make_atari_env(atari_env_name, n_envs=1, seed=0)\r\n",
        "# Frame-stacking with 4 frames. Με 1 frame ο αλγόριθμος ξέρει τη θέση των πραγμάτων, με 2 frames την ταχύτητα, με 3 την επιτάχυνση και με 4 το jerk\r\n",
        "env = VecFrameStack(env, n_stack=4)\r\n",
        "# Test environment must be unique\r\n",
        "test_env = make_atari_env(atari_env_name, n_envs=1, seed=0)\r\n",
        "# Frame-stacking with 4 frames\r\n",
        "test_env = VecFrameStack(test_env, n_stack=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOAzZ7KFoGT6"
      },
      "source": [
        "model_name='dqn-MlpPolicy'\r\n",
        "time_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\r\n",
        "model_log= LOG_DIR + model_name + time_stamp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9yjFtQ4t_IB"
      },
      "source": [
        "dqn_model = DQN('MlpPolicy', env, verbose=1, tensorboard_log=model_log, buffer_size=100000, exploration_final_eps=0.15)\r\n",
        "max_steps=10000\r\n",
        "dqn_model.learn(total_timesteps=max_steps)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Deq0ZgIaoQzn"
      },
      "source": [
        "%tensorboard --logdir {LOG_DIR}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuGvCQgJoRHb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0605adca-6cd9-41cf-da0d-06da187bae45"
      },
      "source": [
        "from stable_baselines3.common.callbacks import StopTrainingOnMaxEpisodes\r\n",
        "callback = StopTrainingOnMaxEpisodes\r\n",
        "mean_reward, std_reward = evaluate_policy(dqn_model, test_env, callback=callback, n_eval_episodes=10)\r\n",
        "print(f\"Eval reward: {mean_reward} (+/-{std_reward})\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Eval reward: 115.0 (+/-22.9128784747792)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDXL1206uOmU"
      },
      "source": [
        "record_video(test_env, dqn_model, video_length=5000, prefix='dqn_BerzerkDeterministic-v4')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXShHm6xuSOG"
      },
      "source": [
        "show_videos(video_path = video_folder, prefix='dqn')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceG9PgkHxA0t"
      },
      "source": [
        "dqn_model.save(\"dqn_BerzerkDeterministic-v4\")\r\n",
        "obs = env.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l41Zv-y05YPI"
      },
      "source": [
        "##DQN-v4 (simple)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyEa8Ujc5cyF"
      },
      "source": [
        "atari_env_name='Berzerk-v4'\r\n",
        "\r\n",
        "# Here we are also multi-worker training (n_envs=4 => 4 environments), The model must support Multi Processing\r\n",
        "env = make_atari_env(atari_env_name, n_envs=1, seed=0)\r\n",
        "# Frame-stacking with 4 frames. Με 1 frame ο αλγόριθμος ξέρει τη θέση των πραγμάτων, με 2 frames την ταχύτητα, με 3 την επιτάχυνση και με 4 το jerk\r\n",
        "env = VecFrameStack(env, n_stack=4)\r\n",
        "# Test environment must be unique\r\n",
        "test_env = make_atari_env(atari_env_name, n_envs=1, seed=0)\r\n",
        "# Frame-stacking with 4 frames\r\n",
        "test_env = VecFrameStack(test_env, n_stack=4)\r\n",
        "\r\n",
        "model_name='dqn-MlpPolicy'\r\n",
        "time_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\r\n",
        "model_log= LOG_DIR + model_name + time_stamp\r\n",
        "\r\n",
        "dqn_model = DQN('MlpPolicy', env, verbose=1, tensorboard_log=model_log, buffer_size=100000, exploration_final_eps=0.15)\r\n",
        "max_steps=10000\r\n",
        "dqn_model.learn(total_timesteps=max_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XI9IJf465zo1"
      },
      "source": [
        "%tensorboard --logdir {LOG_DIR}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIwkFuzL56Ml"
      },
      "source": [
        "from stable_baselines3.common.callbacks import StopTrainingOnMaxEpisodes\r\n",
        "callback = StopTrainingOnMaxEpisodes\r\n",
        "mean_reward, std_reward = evaluate_policy(dqn_model, test_env, callback=callback, n_eval_episodes=10)\r\n",
        "print(f\"Eval reward: {mean_reward} (+/-{std_reward})\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5u3MkpCU5_34"
      },
      "source": [
        "record_video(test_env, dqn_model, video_length=5000, prefix='dqn_Berzerk-v4')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ud5tDPf6Dxw"
      },
      "source": [
        "show_videos(video_path = video_folder, prefix='dqn')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI-MGtNh6JHJ"
      },
      "source": [
        "dqn_model.save(\"dqn_Berzerk-v4\")\r\n",
        "obs = env.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBLfv4KHfqF2"
      },
      "source": [
        "##DQN NoFrameskip-v4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHBRmFlnfzhL"
      },
      "source": [
        "atari_env_name='BerzerkNoFrameskip-v4'\r\n",
        "\r\n",
        "# Here we are also multi-worker training (n_envs=4 => 4 environments), The model must support Multi Processing\r\n",
        "env = make_atari_env(atari_env_name, n_envs=1, seed=0)\r\n",
        "# Frame-stacking with 4 frames. Με 1 frame ο αλγόριθμος ξέρει τη θέση των πραγμάτων, με 2 frames την ταχύτητα, με 3 την επιτάχυνση και με 4 το jerk\r\n",
        "env = VecFrameStack(env, n_stack=4)\r\n",
        "# Test environment must be unique\r\n",
        "test_env = make_atari_env(atari_env_name, n_envs=1, seed=0)\r\n",
        "# Frame-stacking with 4 frames\r\n",
        "test_env = VecFrameStack(test_env, n_stack=4)\r\n",
        "\r\n",
        "model_name='dqn-MlpPolicy'\r\n",
        "time_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\r\n",
        "model_log= LOG_DIR + model_name + time_stamp\r\n",
        "\r\n",
        "dqn_model = DQN('CnnPolicy', env, verbose=1, tensorboard_log=model_log, buffer_size=100000, exploration_final_eps=0.15)\r\n",
        "max_steps= 1000000\r\n",
        "dqn_model.learn(total_timesteps=max_steps)\r\n",
        "\r\n",
        "# %tensorboard --logdir {LOG_DIR}\r\n",
        "\r\n",
        "# from stable_baselines3.common.callbacks import StopTrainingOnMaxEpisodes\r\n",
        "# callback = StopTrainingOnMaxEpisodes\r\n",
        "# mean_reward, std_reward = evaluate_policy(dqn_model, test_env, callback=callback, n_eval_episodes=10)\r\n",
        "# print(f\"Eval reward: {mean_reward} (+/-{std_reward})\")\r\n",
        "\r\n",
        "# record_video(test_env, dqn_model, video_length=5000, prefix='dqn_BerzerkNoFrameskip-v4')\r\n",
        "\r\n",
        "# show_videos(video_path = video_folder, prefix='dqn')\r\n",
        "\r\n",
        "dqn_model.save(\"dqn_BerzerkNoFrameskip-v4\")\r\n",
        "obs = env.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMkI_dnWpiF_"
      },
      "source": [
        "# %tensorboard --logdir {LOG_DIR}\r\n",
        "\r\n",
        "from stable_baselines3.common.callbacks import StopTrainingOnMaxEpisodes\r\n",
        "callback = StopTrainingOnMaxEpisodes\r\n",
        "mean_reward, std_reward = evaluate_policy(dqn_model, test_env, callback=callback, n_eval_episodes=10)\r\n",
        "print(f\"Eval reward: {mean_reward} (+/-{std_reward})\")\r\n",
        "\r\n",
        "record_video(test_env, dqn_model, video_length=5000, prefix='dqn_BerzerkNoFrameskip-v4')\r\n",
        "\r\n",
        "show_videos(video_path = video_folder, prefix='dqn')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waQDhqlDh6ge"
      },
      "source": [
        "##A2C Deterministic-v4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmX0CA9YiBIK"
      },
      "source": [
        "atari_env_name='BerzerkDeterministic-v4'\r\n",
        "\r\n",
        "# Here we are also multi-worker training (n_envs=4 => 4 environments), The model must support Multi Processing\r\n",
        "env = make_atari_env(atari_env_name, n_envs=1, seed=0)\r\n",
        "# Frame-stacking with 4 frames. Με 1 frame ο αλγόριθμος ξέρει τη θέση των πραγμάτων, με 2 frames την ταχύτητα, με 3 την επιτάχυνση και με 4 το jerk\r\n",
        "env = VecFrameStack(env, n_stack=4)\r\n",
        "# Test environment must be unique\r\n",
        "test_env = make_atari_env(atari_env_name, n_envs=1, seed=0)\r\n",
        "# Frame-stacking with 4 frames\r\n",
        "test_env = VecFrameStack(test_env, n_stack=4)\r\n",
        "\r\n",
        "model_name='a2c-MlpPolicy'\r\n",
        "time_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\r\n",
        "model_log= LOG_DIR + model_name + time_stamp\r\n",
        "\r\n",
        "a2c_model = A2C('MlpPolicy', env, verbose=1, tensorboard_log=model_log)\r\n",
        "max_steps=10000\r\n",
        "a2c_model.learn(total_timesteps=max_steps)\r\n",
        "\r\n",
        "%tensorboard --logdir {LOG_DIR}\r\n",
        "\r\n",
        "from stable_baselines3.common.callbacks import StopTrainingOnMaxEpisodes\r\n",
        "callback = StopTrainingOnMaxEpisodes\r\n",
        "mean_reward, std_reward = evaluate_policy(a2c_model, test_env, callback=callback, n_eval_episodes=10)\r\n",
        "print(f\"Eval reward: {mean_reward} (+/-{std_reward})\")\r\n",
        "\r\n",
        "record_video(test_env, a2c_model, video_length=5000, prefix='a2c_BerzerkDeterministic-v4')\r\n",
        "\r\n",
        "show_videos(video_path = video_folder, prefix='a2c')\r\n",
        "\r\n",
        "a2c_model.save(\"a2c_BerzerkDeterministic-v4\")\r\n",
        "obs = env.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPf-jF2NnAmZ"
      },
      "source": [
        "##A2C-v4 (simple)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9yZG7_pnLg-"
      },
      "source": [
        "atari_env_name='Berzerk-v4'\r\n",
        "\r\n",
        "# Here we are also multi-worker training (n_envs=4 => 4 environments), The model must support Multi Processing\r\n",
        "env = make_atari_env(atari_env_name, n_envs=1, seed=0)\r\n",
        "# Frame-stacking with 4 frames. Με 1 frame ο αλγόριθμος ξέρει τη θέση των πραγμάτων, με 2 frames την ταχύτητα, με 3 την επιτάχυνση και με 4 το jerk\r\n",
        "env = VecFrameStack(env, n_stack=4)\r\n",
        "# Test environment must be unique\r\n",
        "test_env = make_atari_env(atari_env_name, n_envs=1, seed=0)\r\n",
        "# Frame-stacking with 4 frames\r\n",
        "test_env = VecFrameStack(test_env, n_stack=4)\r\n",
        "\r\n",
        "model_name='a2c-MlpPolicy'\r\n",
        "time_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\r\n",
        "model_log= LOG_DIR + model_name + time_stamp\r\n",
        "\r\n",
        "a2c_model = A2C('MlpPolicy', env, verbose=1, tensorboard_log=model_log)\r\n",
        "max_steps=10000\r\n",
        "a2c_model.learn(total_timesteps=max_steps)\r\n",
        "\r\n",
        "# %tensorboard --logdir {LOG_DIR}\r\n",
        "\r\n",
        "# from stable_baselines3.common.callbacks import StopTrainingOnMaxEpisodes\r\n",
        "# callback = StopTrainingOnMaxEpisodes\r\n",
        "# mean_reward, std_reward = evaluate_policy(a2c_model, test_env, callback=callback, n_eval_episodes=10)\r\n",
        "# print(f\"Eval reward: {mean_reward} (+/-{std_reward})\")\r\n",
        "\r\n",
        "# record_video(test_env, a2c_model, video_length=5000, prefix='a2c_Berzerk-v4')\r\n",
        "\r\n",
        "# show_videos(video_path = video_folder, prefix='a2c')\r\n",
        "\r\n",
        "a2c_model.save(\"a2c_Berzerk-v4\")\r\n",
        "obs = env.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uK1fdCEvnuKE"
      },
      "source": [
        "##A2C NoFrameskip-v4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oawdN8Q2n28O"
      },
      "source": [
        "atari_env_name='BerzerkNoFrameskip-v4'\r\n",
        "\r\n",
        "# Here we are also multi-worker training (n_envs=4 => 4 environments), The model must support Multi Processing\r\n",
        "env = make_atari_env(atari_env_name, n_envs=1, seed=0)\r\n",
        "# Frame-stacking with 4 frames. Με 1 frame ο αλγόριθμος ξέρει τη θέση των πραγμάτων, με 2 frames την ταχύτητα, με 3 την επιτάχυνση και με 4 το jerk\r\n",
        "env = VecFrameStack(env, n_stack=4)\r\n",
        "# Test environment must be unique\r\n",
        "test_env = make_atari_env(atari_env_name, n_envs=1, seed=0)\r\n",
        "# Frame-stacking with 4 frames\r\n",
        "test_env = VecFrameStack(test_env, n_stack=4)\r\n",
        "\r\n",
        "model_name='a2c-MlpPolicy'\r\n",
        "time_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\r\n",
        "model_log= LOG_DIR + model_name + time_stamp\r\n",
        "\r\n",
        "a2c_model = A2C('MlpPolicy', env, verbose=1, tensorboard_log=model_log, buffer_size=100000, exploration_final_eps=0.15)\r\n",
        "max_steps=10000\r\n",
        "a2c_model.learn(total_timesteps=max_steps)\r\n",
        "\r\n",
        "%tensorboard --logdir {LOG_DIR}\r\n",
        "\r\n",
        "from stable_baselines3.common.callbacks import StopTrainingOnMaxEpisodes\r\n",
        "callback = StopTrainingOnMaxEpisodes\r\n",
        "mean_reward, std_reward = evaluate_policy(a2c_model, test_env, callback=callback, n_eval_episodes=10)\r\n",
        "print(f\"Eval reward: {mean_reward} (+/-{std_reward})\")\r\n",
        "\r\n",
        "record_video(test_env, a2c_model, video_length=5000, prefix='a2c_BerzerkNoFrameskip-v4')\r\n",
        "\r\n",
        "show_videos(video_path = video_folder, prefix='a2c')\r\n",
        "\r\n",
        "a2c_model.save(\"a2c_BerzerkNoFrameskip-v4\")\r\n",
        "obs = env.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AaLWpGWpR_A"
      },
      "source": [
        "##PPO Deterministic-v4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ny8m94owpX0l"
      },
      "source": [
        "atari_env_name='BerzerkDeterministic-v4'\r\n",
        "\r\n",
        "# Here we are also multi-worker training (n_envs=4 => 4 environments), The model must support Multi Processing\r\n",
        "env = make_atari_env(atari_env_name, n_envs=1, seed=0)\r\n",
        "# Frame-stacking with 4 frames. Με 1 frame ο αλγόριθμος ξέρει τη θέση των πραγμάτων, με 2 frames την ταχύτητα, με 3 την επιτάχυνση και με 4 το jerk\r\n",
        "env = VecFrameStack(env, n_stack=4)\r\n",
        "# Test environment must be unique\r\n",
        "test_env = make_atari_env(atari_env_name, n_envs=1, seed=0)\r\n",
        "# Frame-stacking with 4 frames\r\n",
        "test_env = VecFrameStack(test_env, n_stack=4)\r\n",
        "\r\n",
        "model_name='ppo-MlpPolicy'\r\n",
        "time_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\r\n",
        "model_log= LOG_DIR + model_name + time_stamp\r\n",
        "\r\n",
        "ppo_model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=model_log, )\r\n",
        "max_steps=10000\r\n",
        "ppo_model.learn(total_timesteps=max_steps)\r\n",
        "\r\n",
        "%tensorboard --logdir {LOG_DIR}\r\n",
        "\r\n",
        "from stable_baselines3.common.callbacks import StopTrainingOnMaxEpisodes\r\n",
        "callback = StopTrainingOnMaxEpisodes\r\n",
        "mean_reward, std_reward = evaluate_policy(ppo_model, test_env, callback=callback, n_eval_episodes=10)\r\n",
        "print(f\"Eval reward: {mean_reward} (+/-{std_reward})\")\r\n",
        "\r\n",
        "record_video(test_env, ppo_model, video_length=5000, prefix='ppo_BerzerkDeterministic-v4')\r\n",
        "\r\n",
        "show_videos(video_path = video_folder, prefix='ppo')\r\n",
        "\r\n",
        "ppo_model.save(\"a2c_BerzerkDeterministic-v4\")\r\n",
        "obs = env.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8btcrXLRqUwI"
      },
      "source": [
        "##PPO-v4 (simple)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9E12Pe5qZ-h"
      },
      "source": [
        "atari_env_name='Berzerk-v4'\r\n",
        "\r\n",
        "# Here we are also multi-worker training (n_envs=4 => 4 environments), The model must support Multi Processing\r\n",
        "env = make_atari_env(atari_env_name, n_envs=1, seed=0)\r\n",
        "# Frame-stacking with 4 frames. Με 1 frame ο αλγόριθμος ξέρει τη θέση των πραγμάτων, με 2 frames την ταχύτητα, με 3 την επιτάχυνση και με 4 το jerk\r\n",
        "env = VecFrameStack(env, n_stack=4)\r\n",
        "# Test environment must be unique\r\n",
        "test_env = make_atari_env(atari_env_name, n_envs=1, seed=0)\r\n",
        "# Frame-stacking with 4 frames\r\n",
        "test_env = VecFrameStack(test_env, n_stack=4)\r\n",
        "\r\n",
        "model_name='ppo-MlpPolicy'\r\n",
        "time_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\r\n",
        "model_log= LOG_DIR + model_name + time_stamp\r\n",
        "\r\n",
        "ppo_model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=model_log)\r\n",
        "max_steps=10000\r\n",
        "ppo_model.learn(total_timesteps=max_steps)\r\n",
        "\r\n",
        "# %tensorboard --logdir {LOG_DIR}\r\n",
        "\r\n",
        "# from stable_baselines3.common.callbacks import StopTrainingOnMaxEpisodes\r\n",
        "# callback = StopTrainingOnMaxEpisodes\r\n",
        "# mean_reward, std_reward = evaluate_policy(ppo_model, test_env, callback=callback, n_eval_episodes=10)\r\n",
        "# print(f\"Eval reward: {mean_reward} (+/-{std_reward})\")\r\n",
        "\r\n",
        "# record_video(test_env, ppo_model, video_length=5000, prefix='ppo_Berzerk-v4')\r\n",
        "\r\n",
        "# show_videos(video_path = video_folder, prefix='ppo')\r\n",
        "\r\n",
        "ppo_model.save(\"ppo_Berzerk-v4\")\r\n",
        "obs = env.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wZKx1ZIq2mQ"
      },
      "source": [
        "##PPO NoFrameskip-v4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f01uP97IrB24"
      },
      "source": [
        "atari_env_name='BerzerkNoFrameskip-v4'\r\n",
        "\r\n",
        "# Here we are also multi-worker training (n_envs=4 => 4 environments), The model must support Multi Processing\r\n",
        "env = make_atari_env(atari_env_name, n_envs=1, seed=0)\r\n",
        "# Frame-stacking with 4 frames. Με 1 frame ο αλγόριθμος ξέρει τη θέση των πραγμάτων, με 2 frames την ταχύτητα, με 3 την επιτάχυνση και με 4 το jerk\r\n",
        "env = VecFrameStack(env, n_stack=4)\r\n",
        "# Test environment must be unique\r\n",
        "test_env = make_atari_env(atari_env_name, n_envs=1, seed=0)\r\n",
        "# Frame-stacking with 4 frames\r\n",
        "test_env = VecFrameStack(test_env, n_stack=4)\r\n",
        "\r\n",
        "model_name='ppo-MlpPolicy'\r\n",
        "time_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\r\n",
        "model_log= LOG_DIR + model_name + time_stamp\r\n",
        "\r\n",
        "ppo_model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=model_log, buffer_size=100000, exploration_final_eps=0.15)\r\n",
        "max_steps=10000\r\n",
        "ppo_model.learn(total_timesteps=max_steps)\r\n",
        "\r\n",
        "%tensorboard --logdir {LOG_DIR}\r\n",
        "\r\n",
        "from stable_baselines3.common.callbacks import StopTrainingOnMaxEpisodes\r\n",
        "callback = StopTrainingOnMaxEpisodes\r\n",
        "mean_reward, std_reward = evaluate_policy(ppo_model, test_env, callback=callback, n_eval_episodes=10)\r\n",
        "print(f\"Eval reward: {mean_reward} (+/-{std_reward})\")\r\n",
        "\r\n",
        "record_video(test_env, ppo_model, video_length=5000, prefix='ppo_BerzerkNoFrameskip-v4')\r\n",
        "\r\n",
        "show_videos(video_path = video_folder, prefix='ppo')\r\n",
        "\r\n",
        "ppo_model.save(\"ppo_BerzerkNoFrameskip-v4\")\r\n",
        "obs = env.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVr7MDa-gco5"
      },
      "source": [
        "## Random Agent\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHAdnSc2gcHc"
      },
      "source": [
        "import numpy as np\r\n",
        "# Here we are also multi-worker training (n_envs=4 => 4 environments), The model must support Multi Processing\r\n",
        "env = make_atari_env(atari_env_name, n_envs=1, seed=0)\r\n",
        "# Frame-stacking with 4 frames. Με 1 frame ο αλγόριθμος ξέρει τη θέση των πραγμάτων, με 2 frames την ταχύτητα, με 3 την επιτάχυνση και με 4 το jerk\r\n",
        "env = VecFrameStack(env, n_stack=4)\r\n",
        "obs = env.reset()\r\n",
        "n_steps = 1000\r\n",
        "total_reward = 0\r\n",
        "episodes = 0\r\n",
        "for _ in range(n_steps):\r\n",
        "    action = np.array([env.action_space.sample()])\r\n",
        "    obs, reward, done, info = env.step(action)\r\n",
        "    total_reward += reward[0]\r\n",
        "    if done:\r\n",
        "      print(\"Episode:{} Score:{}\".format(episodes + 1,total_reward))\r\n",
        "      obs = env.reset()\r\n",
        "      episodes += 1\r\n",
        "    if episodes == 10:\r\n",
        "      break\r\n",
        "      print(\"Final Score:{} Number of Episodes:{}\".format(total_reward, episodes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRjHnh3SFVXk"
      },
      "source": [
        "## Βελτιστοποιηση agent A2C"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8LbOpI-FU_J"
      },
      "source": [
        "atari_env_name='Berzerk-v4'\r\n",
        "\r\n",
        "# Here we are also multi-worker training (n_envs=4 => 4 environments), The model must support Multi Processing\r\n",
        "env = make_atari_env(atari_env_name, n_envs=4, seed=0)\r\n",
        "# Frame-stacking with 4 frames. Με 1 frame ο αλγόριθμος ξέρει τη θέση των πραγμάτων, με 2 frames την ταχύτητα, με 3 την επιτάχυνση και με 4 το jerk\r\n",
        "env = VecFrameStack(env, n_stack=4)\r\n",
        "# Test environment must be unique\r\n",
        "test_env = make_atari_env(atari_env_name, n_envs=4, seed=0)\r\n",
        "# Frame-stacking with 4 frames\r\n",
        "test_env = VecFrameStack(test_env, n_stack=4)\r\n",
        "\r\n",
        "model_name='a2c-MlpPolicy'\r\n",
        "time_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\r\n",
        "model_log= LOG_DIR + model_name + time_stamp\r\n",
        "\r\n",
        "a2c_model = A2C('MlpPolicy', env, verbose=1, tensorboard_log=model_log)\r\n",
        "max_steps=1000000\r\n",
        "a2c_model.learn(total_timesteps=max_steps)\r\n",
        "\r\n",
        "a2c_model.save(\"a2c_Berzerk-v4\")\r\n",
        "obs = env.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJ1ua4qMFksj"
      },
      "source": [
        "%tensorboard --logdir {LOG_DIR}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3kel-FvSUyQ",
        "outputId": "30310a47-eb19-4ee3-a00a-1c97143771f2"
      },
      "source": [
        "from stable_baselines3.common.callbacks import StopTrainingOnMaxEpisodes\r\n",
        "callback = StopTrainingOnMaxEpisodes\r\n",
        "test_env = make_atari_env(atari_env_name, n_envs=1, seed=0)\r\n",
        "# Frame-stacking with 4 frames\r\n",
        "test_env = VecFrameStack(test_env, n_stack=4)\r\n",
        "mean_reward, std_reward = evaluate_policy(a2c_model, test_env, callback=callback, n_eval_episodes=10)\r\n",
        "print(f\"Eval reward: {mean_reward} (+/-{std_reward})\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Eval reward: 610.0 (+/-73.48469228349535)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43Wk6g7NFlug"
      },
      "source": [
        "record_video(test_env, a2c_model, video_length=5000, prefix='a2c_Berzerk-v4')\r\n",
        "\r\n",
        "show_videos(video_path = video_folder, prefix='a2c')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxU2rD-QUh-y"
      },
      "source": [
        "### Πολύ μικρό learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzyyncPOUhT8"
      },
      "source": [
        "atari_env_name='Berzerk-v4'\r\n",
        "\r\n",
        "# Here we are also multi-worker training (n_envs=4 => 4 environments), The model must support Multi Processing\r\n",
        "env = make_atari_env(atari_env_name, n_envs=4, seed=0)\r\n",
        "# Frame-stacking with 4 frames. Με 1 frame ο αλγόριθμος ξέρει τη θέση των πραγμάτων, με 2 frames την ταχύτητα, με 3 την επιτάχυνση και με 4 το jerk\r\n",
        "env = VecFrameStack(env, n_stack=4)\r\n",
        "# Test environment must be unique\r\n",
        "test_env = make_atari_env(atari_env_name, n_envs=4, seed=0)\r\n",
        "# Frame-stacking with 4 frames\r\n",
        "test_env = VecFrameStack(test_env, n_stack=4)\r\n",
        "\r\n",
        "model_name='a2c-MlpPolicy'\r\n",
        "time_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\r\n",
        "model_log= LOG_DIR + model_name + time_stamp\r\n",
        "\r\n",
        "a2c_model = A2C('MlpPolicy', env, verbose=1, tensorboard_log=model_log, learning_rate=0.00001)\r\n",
        "max_steps=200000\r\n",
        "a2c_model.learn(total_timesteps=max_steps)\r\n",
        "\r\n",
        "a2c_model.save(\"a2c_Berzerk-v4\")\r\n",
        "obs = env.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4vS2-sUXUF6",
        "outputId": "c95b14fe-aef4-46a6-f49c-1b4826c6b883"
      },
      "source": [
        "from stable_baselines3.common.callbacks import StopTrainingOnMaxEpisodes\r\n",
        "callback = StopTrainingOnMaxEpisodes\r\n",
        "test_env = make_atari_env(atari_env_name, n_envs=1, seed=0)\r\n",
        "# Frame-stacking with 4 frames\r\n",
        "test_env = VecFrameStack(test_env, n_stack=4)\r\n",
        "mean_reward, std_reward = evaluate_policy(a2c_model, test_env, callback=callback, n_eval_episodes=10)\r\n",
        "print(f\"Eval reward: {mean_reward} (+/-{std_reward})\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Eval reward: 180.0 (+/-67.82329983125268)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J_nL8HgXbar"
      },
      "source": [
        "record_video(test_env, a2c_model, video_length=5000, prefix='a2c_Berzerk-v4')\r\n",
        "\r\n",
        "show_videos(video_path = video_folder, prefix='a2c')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcUaHIWAXinT"
      },
      "source": [
        "### Μεγάλο gamma coefficient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19me-B1eXh_x"
      },
      "source": [
        "atari_env_name='Berzerk-v4'\r\n",
        "\r\n",
        "# Here we are also multi-worker training (n_envs=4 => 4 environments), The model must support Multi Processing\r\n",
        "env = make_atari_env(atari_env_name, n_envs=4, seed=0)\r\n",
        "# Frame-stacking with 4 frames. Με 1 frame ο αλγόριθμος ξέρει τη θέση των πραγμάτων, με 2 frames την ταχύτητα, με 3 την επιτάχυνση και με 4 το jerk\r\n",
        "env = VecFrameStack(env, n_stack=4)\r\n",
        "# Test environment must be unique\r\n",
        "test_env = make_atari_env(atari_env_name, n_envs=4, seed=0)\r\n",
        "# Frame-stacking with 4 frames\r\n",
        "test_env = VecFrameStack(test_env, n_stack=4)\r\n",
        "\r\n",
        "model_name='a2c-MlpPolicy'\r\n",
        "time_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\r\n",
        "model_log= LOG_DIR + model_name + time_stamp\r\n",
        "\r\n",
        "a2c_model = A2C('MlpPolicy', env, verbose=1, tensorboard_log=model_log, gamma = .90, gae_lambda=.9)\r\n",
        "max_steps=200000\r\n",
        "a2c_model.learn(total_timesteps=max_steps)\r\n",
        "\r\n",
        "a2c_model.save(\"a2c_Berzerk-v4\")\r\n",
        "obs = env.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8USMncHPX6QX",
        "outputId": "956417c3-b75c-42fd-dde0-637045ee4ab2"
      },
      "source": [
        "from stable_baselines3.common.callbacks import StopTrainingOnMaxEpisodes\r\n",
        "callback = StopTrainingOnMaxEpisodes\r\n",
        "test_env = make_atari_env(atari_env_name, n_envs=1, seed=0)\r\n",
        "# Frame-stacking with 4 frames\r\n",
        "test_env = VecFrameStack(test_env, n_stack=4)\r\n",
        "mean_reward, std_reward = evaluate_policy(a2c_model, test_env, callback=callback, n_eval_episodes=10)\r\n",
        "print(f\"Eval reward: {mean_reward} (+/-{std_reward})\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Eval reward: 365.0 (+/-150.08331019803634)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPYy-8P3X7sl"
      },
      "source": [
        "record_video(test_env, a2c_model, video_length=5000, prefix='a2c_Berzerk-v4')\r\n",
        "\r\n",
        "show_videos(video_path = video_folder, prefix='a2c')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-lJbqVMdlOm"
      },
      "source": [
        "### Μεγάλο learning rate με μικρό gamma coefficient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJxGmV0BdpNw"
      },
      "source": [
        "atari_env_name='Berzerk-v4'\r\n",
        "\r\n",
        "# Here we are also multi-worker training (n_envs=4 => 4 environments), The model must support Multi Processing\r\n",
        "env = make_atari_env(atari_env_name, n_envs=4, seed=0)\r\n",
        "# Frame-stacking with 4 frames. Με 1 frame ο αλγόριθμος ξέρει τη θέση των πραγμάτων, με 2 frames την ταχύτητα, με 3 την επιτάχυνση και με 4 το jerk\r\n",
        "env = VecFrameStack(env, n_stack=4)\r\n",
        "# Test environment must be unique\r\n",
        "test_env = make_atari_env(atari_env_name, n_envs=4, seed=0)\r\n",
        "# Frame-stacking with 4 frames\r\n",
        "test_env = VecFrameStack(test_env, n_stack=4)\r\n",
        "\r\n",
        "model_name='a2c-MlpPolicy'\r\n",
        "time_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\r\n",
        "model_log= LOG_DIR + model_name + time_stamp\r\n",
        "\r\n",
        "a2c_model = A2C('MlpPolicy', env, verbose=1, tensorboard_log=model_log, gamma = .90, learning_rate=0.003)\r\n",
        "max_steps=200000\r\n",
        "a2c_model.learn(total_timesteps=max_steps)\r\n",
        "\r\n",
        "a2c_model.save(\"a2c_Berzerk-v4\")\r\n",
        "obs = env.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Awdc1FT6d1rt",
        "outputId": "5ebe5a03-acd7-4bc0-aad6-1982c0b4f918"
      },
      "source": [
        "from stable_baselines3.common.callbacks import StopTrainingOnMaxEpisodes\r\n",
        "callback = StopTrainingOnMaxEpisodes\r\n",
        "test_env = make_atari_env(atari_env_name, n_envs=1, seed=0)\r\n",
        "# Frame-stacking with 4 frames\r\n",
        "test_env = VecFrameStack(test_env, n_stack=4)\r\n",
        "mean_reward, std_reward = evaluate_policy(a2c_model, test_env, callback=callback, n_eval_episodes=10)\r\n",
        "print(f\"Eval reward: {mean_reward} (+/-{std_reward})\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Eval reward: 555.0 (+/-127.37739202856997)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARakFhefdyoc"
      },
      "source": [
        "record_video(test_env, a2c_model, video_length=5000, prefix='a2c_Berzerk-v4')\r\n",
        "\r\n",
        "show_videos(video_path = video_folder, prefix='a2c')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gli0ymMkh_7"
      },
      "source": [
        "### CNN Policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN2Ajhcikka1"
      },
      "source": [
        "atari_env_name='Berzerk-v4'\r\n",
        "\r\n",
        "# Here we are also multi-worker training (n_envs=4 => 4 environments), The model must support Multi Processing\r\n",
        "env = make_atari_env(atari_env_name, n_envs=4, seed=0)\r\n",
        "# Frame-stacking with 4 frames. Με 1 frame ο αλγόριθμος ξέρει τη θέση των πραγμάτων, με 2 frames την ταχύτητα, με 3 την επιτάχυνση και με 4 το jerk\r\n",
        "env = VecFrameStack(env, n_stack=4)\r\n",
        "# Test environment must be unique\r\n",
        "test_env = make_atari_env(atari_env_name, n_envs=4, seed=0)\r\n",
        "# Frame-stacking with 4 frames\r\n",
        "test_env = VecFrameStack(test_env, n_stack=4)\r\n",
        "\r\n",
        "model_name='a2c-MlpPolicy'\r\n",
        "time_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\r\n",
        "model_log= LOG_DIR + model_name + time_stamp\r\n",
        "\r\n",
        "a2c_model = A2C('CnnPolicy', env, verbose=1, tensorboard_log=model_log)\r\n",
        "max_steps=200000\r\n",
        "a2c_model.learn(total_timesteps=max_steps)\r\n",
        "\r\n",
        "a2c_model.save(\"a2c_Berzerk-v4\")\r\n",
        "obs = env.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cgagEqQkxmq",
        "outputId": "de06a111-e49d-43a2-93cf-efec80a109e3"
      },
      "source": [
        "from stable_baselines3.common.callbacks import StopTrainingOnMaxEpisodes\r\n",
        "callback = StopTrainingOnMaxEpisodes\r\n",
        "test_env = make_atari_env(atari_env_name, n_envs=1, seed=0)\r\n",
        "# Frame-stacking with 4 frames\r\n",
        "test_env = VecFrameStack(test_env, n_stack=4)\r\n",
        "mean_reward, std_reward = evaluate_policy(a2c_model, test_env, callback=callback, n_eval_episodes=10)\r\n",
        "print(f\"Eval reward: {mean_reward} (+/-{std_reward})\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Eval reward: 612.0 (+/-122.45815611873306)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsF-ONQvk1rO"
      },
      "source": [
        "record_video(test_env, a2c_model, video_length=5000, prefix='a2c_Berzerk-v4')\r\n",
        "\r\n",
        "show_videos(video_path = video_folder, prefix='a2c')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}